COMP90049 Report
Geolocation of Tweets with Machine Learning


 
1.	Introduction

    This report discusses the approaches, observations and analysis of using feature engineering and machine learning to classify geolocation of tweets. Bigrams strategy and external resources method are used to generate features. Zero-R, decision tree, Naïve Bayes and Support Vector Machine methods are applied in the machine learning process. There are two conclusions reached: better features should be generated for higher performance in identifying geolocation of tweets; and Random Forest method performs the best with engineered features and an intermediate computation cost in classification. 

2.	Feature Engineering
    In this section, we use the performance of different datasets under Naïve Bayes for analysis purposes because of its cheap computation cost and it’s easy to scale. 
2.1.	 Bi-gram

   By using the best35-no-id features, bi-grams  were generated from pairwise combinations of unigram, and a bi-gram is considered useful only when its MI is greater than both of its unigrams’ MI. One bi-gram, “Bellevue bill”, was generated from this method, and was added to the best35-without-id as a new feature set. In this case, only when both components of the bi-gram appear in the text, its space vector in arff file gets an increment. Ron Bekkerman, James Allan (2003)
    The features generated by this method is easy to compute and statistically reliable. Because by calculating MI, it theoretically captures the “usefulness” of the feature in classification. The result is however surprising. The correctness rate for Naïve Bayes, 28.7515%, is exactly the same as the best35-without-id, even when “Bellevue bill” bigram has a larger MI than both of its components, which theoretically makes the feature set more “useful” in classification. The reason behind this is that, although bigram forces both its components to appear at the same time, both unigrams and bigrams in this context are just examining the appearance of the features, which shows that while the MI being high, it may not be actually improves the result. 

2.2	 Identify Location Indicative Words from External Resources

In this method, location indicative words(LIW) of the five cities are the targets to be found so that by checking the existence of LIWs, the system can identify the geolocation of tweets. Bo, H., Cook, P., Baldwin, T. (2012). To achieve this goal, wiki pages  for the five cities act as the external resources to generate LIW. Wikipedia is chosen because it has a comprehensive introduction in all different aspects of the cities like tourist attractions, special festivals, people, histories and sports. After finding the high frequency words (frequency>5) in each document and removing duplicates between documents, the outputted 400 features are ranked through feature selection by information gain in Weka . Using zero as a threshold, it leaves 91 attributes as the final result. By combining these 91 attributes and best35-without-id, the correctness rate for Naïve Bayes improves to be 28.772%. In this context, the features are modelling the appearance of tokens in raw texts. Therefore, with more LIW, it is easier for the system to perform classification. 
The disadvantage of this method is, there is no way to tell whether the resource can cover the most number of possible LIW for the 5 cities and with more resource input, the performance can be further improved. Also, it should be pointed out that by using information gain, the feature set is more in favour of Random Forest method.

2.3	 Conclusion on Feature Engineering

Through the experiments in this stage, new features were added to the best35-without-id feature set. The best35-without-id set was calculated by MI which is an indication of how useful one feature is to predict the class. Bi-gram strategy is not helping with classifying data although it has a high MI. But by looking for LIW, some improvements were seen as the resulting correctness rate increased by a small amount. Further improvements can be made through importing more reliable external resources. 
Also, the performance is highly limited if the features are only capturing the feature appearance frequency in the tweets and may cause overfitting. It would be more useful if techniques of feature engineering other than the token level techniques can be applied, such as looking for syntactic relationships and using semantic generalisations.
From knowledge gained in Project 1, aspects of tweets that might be useful to predict the geolocation include both the meta-data and the content itself. Although meta-data is not provided in this project, it can actually be a complementary tool to aid with the geolocation classification. Using UserID and length of tweets may also be helpful with classification.

3	Classification by Weka

In this section, the performance, the reasons behind the performance, advantages and reason for using it, are discussed. A summarization of their performance is showed in table 1 below. In this section, the dataset used is the new feature set produced from section 2.2. Although it is biased towards random forest model, there is still improvement observed.   

Dataset and Method	Zero-R	Naïve Bayes	Random Forest 	SVM 
Best35-without id	  26.1615	28.7515	      30.6441      30.5478
New Feature set	    26.1615	28.772	      31.1898	     30.9549

Table 1: Summarization of Correctly Classified rate in %

3.1	Baseline Method: 0-R

The correctness rate of 0-R remains to be 26.16% in this experiment because it only looks at the classification label of training dataset, and classifies all tweets in development dataset as the most frequently appeared outcome. Any other method needs to outperform this to be considered as improvement for this problem.

3.2	 Naïve Bayes

First, the method is fast and easy to compute. It assumes that all features are independent given the classification label, so each feature is considered to contribute to the result independently. Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).
Improvement is observed in this case because the added features are contributing and giving more information of what a tweet content may look like based on the classification. Theoretically, naïve Bayes is not suitable when features are redundant/useless as it calculates the weighted average of all to make classification. So when there are redundant features, it will perform worse where the redundant attributes will be given more weight. But before the new feature set was filtered by feature selection, it has the same NB rate as now, even 10 redundant attributes were removed. This may be explained by the much larger test data size than the redundant attributes portion in total feature set.

3.3	 Random Tree  and Random Forest

Random Forest receives the highest correctness rate, 31.19%, over the other 3 methods, and also better than random tree. This is because in random forest construction, the random selection of subset of features and data (tree bagging) gives it a greater capability to apply to unseen data to avoid overfitting. Plus, the features generated by information gain ranking is in favour of random forest model too. It is also outperforming the other methods here possibly because this is a multi-class classification problem which SVM is less suitable for. The method may be further refined if a higher number of iterations is used. For example, when 20 iterations are used for best35-without-id feature set, it produces 30.644% while given 50 iterations, it increases to 30.647%. 

3.4	 Support Vector Machine

While SVM being a binary classifier traditionally, Weka-SMO uses a 1-vs-1 method for multi-class classification. It took the longest time and largest heap space for Weka to generate the result, which means time-consuming and highly costly to run, especially in this case over 100 attributes were used. The reason for applying it here is to reduce overfitting, because SVM uses large margin separate planes, it has a better generalisation and can be applied better to unseen data.  The resulting correctness rate is 30.95% better than NB as it generalises better. Some tuning may be applied to improve the performance such as changing the kernel function. 


Metric/Method	           Naïve Bayes	Random Forest	SVM
Weighted. Avg Precision	  0.623      	0.530      	0.532      
Weighted. Avg Recall	    0.288     	0.312      	0.310
F-Measure	                0.169	      0.225     	0.220
Correctness rate	       28.772%	   31.1898%	   30.9549%

Table 2: Evaluation Metrics of 3 methods.

3.5	 Evaluation Metrics

    All 3 methods received a much higher precision than recall, which shows that the classification results are more than half correct but not so complete as recalls are around 0.3. Since precision and recall are measuring two different aspects of classification result, by looking at F-measures, it can be observed that they are consistent with their rank in correctness rate as table 2 shows where Random Forest gives best result.

4	Conclusion

From the experiments and discussion above, the engineered features cannot sufficiently solve the problem of geolocation classification of tweets because they only capture the appearance of words. Among models trained using Naïve Bayes, Random Forest or SVM, Random Forest had the best performance over the other methods with an intermediate expensive cost. It is however expected to perform better with feature sets that models more information and not purely tweet-content-based, and improved using feature selection using subset method which is less biased towards a single method. 








 
 
 

 
References 

Bekkerman, R., & Allan, J. (2004). Using bigrams in text categorization. Technical Report IR-408, Centre of Intelligent Information Retrieval, UMass 
Amherst.
Bo, H., Cook, P., & Baldwin, T. (2012). Geolocation prediction in social media data by finding location indicative words. Proceedings of COLING 2012: Technical Papers, 1045-1062.
Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). Tackling the poor assumptions of naive bayes text classifiers. In ICML (Vol. 3, pp. 
616-623).




 
APPENDIX

Input: D – training set of documents of size Nd;
W – set of all the distinct unigrams in D;
C – set of document categories in D of size Nc;
ku – threshold on unigrams;
kb – threshold on bigrams
Output: New representation of documents in dataset
Procedure Feature Induction
1: For all ci 2 C do
2: Wi Ã list of w 2 W sorted by MI(w; ci)
3: Ui Ã set of ku top ranked unigrams from Wi
4: U Ã
SNc
i=1 Ui // All top-ranked unigrams
5: For all di 2 D do
6: LUi Ã list of unigrams in di that occur in U
7: Pi Ã ;
8: For j = 2, . . . , number of unigrams in LUi do
9: Let bj = (LUi[j ¡ 1], LUi[j]) be the j-th bigram over LUi
10: Pi Ã Pi [ bj
11: F Ã U [ (
SNd
i=1 Pi) // Top-ranked unigrams and their consequent pairs (bigrams)
12: For all ci 2 C do
13: Fi Ã list of f 2 F sorted by MI(f; ci)
14: LBi Ã empty list
15: For j = 1, . . . , number of bigrams in Fi do
16: Let bj = (wj1, wj2) be the j-th bigram from Fi
17: If MI(bj ; ci) > max(MI(wj1; ci);MI(wj2; ci)) then
18: Push bj to LBi
19: Bi Ãset of kb top ranked bigrams from LBi
20: B Ã
SNc
i=1 Bi // All top ranked bigrams that are “better” than both their components
21: For all di 2 D do
22: BOWi Ã bag of unigrams of di
23: Represent di as BOWi [ (Pi \ B)























